{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from re import finditer\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import string\n",
    "import nltk\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "    nltk.data.find('taggers/averaged_perceptron_tagger')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "punct = set(string.punctuation)\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lambda_unpack(f):\n",
    "    return lambda args: f(*args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "contractions = {\n",
    "\"ain't\": \"am not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"needn't\": \"need not\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there had\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'll\": \"we will\",\n",
    "\"We'll\": \"We will\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who's\": \"who is\",\n",
    "\"won't\": \"will not\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you're\": \"you are\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleantext(text):\n",
    "    rep = {\"\\n\": \" \", \"\\t\": \" \", \"--\": \" \", \"--R\": \" \", \";\": \" \",\"(\":\" \",\")\":\" \",\"[\":\" \",\"]\":\" \",\",\":\" \",\"#\":\" \"}\n",
    "    substrs = sorted(rep, key=len, reverse=True)\n",
    "\n",
    "    # Create a big OR regex that matches any of the substrings to replace\n",
    "    regexp = re.compile('|'.join(map(re.escape, substrs)))\n",
    "\n",
    "    # For each match, look up the new string in the replacements\n",
    "    text =  regexp.sub(lambda match: rep[match.group(0)], text)\n",
    "\n",
    "    text = replaceContractions(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def replaceContractions(text):\n",
    "    c_filt_text = ''\n",
    "    for word in word_tokenize(text):\n",
    "        if word in contractions:\n",
    "            c_filt_text = c_filt_text+' '+contractions[word]\n",
    "        else:\n",
    "            c_filt_text = c_filt_text+' '+word\n",
    "    return c_filt_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def extract_candidate_chunk(text_all, grammar=r'KT: {<(CD)|(DT)|(JJR)>*( (<NN>+ <NN.>+)|((<JJ>|<NN>) <NN>)| ((<JJ>|<NN>)+|((<JJ>|<NN>)* (<NN> <NN.>)? (<JJ>|<NN>)*) <NN.>)) <VB.>*}'):\n",
    "    chunker = nltk.RegexpParser(grammar)\n",
    "    candidates_all = []\n",
    "    key_pos = []\n",
    "    for text in sent_tokenize(text_all):\n",
    "        if text!=\" \" and text!=\"\":\n",
    "            #print (text,[word_tokenize (sent) for sent in sent_tokenize (text)])\n",
    "            tagged_sents = nltk.pos_tag ([word_tokenize (sent) for sent in sent_tokenize (text)] [0])\n",
    "            all_chunks = itertools.chain.from_iterable([nltk.chunk.tree2conlltags(chunker.parse(tagged_sents)) for tagged_sent in tagged_sents])\n",
    "            candidates = [' '.join(word for word,pos, chunk in group).lower() for key,group in itertools.groupby(all_chunks, lambda_unpack(lambda word,pos,chunk: chunk !='O')) if key]\n",
    "            candidates_all += candidates\n",
    "    valid_key = list(set([cand for cand in candidates_all if cand not in stop_words and not all(char in punct for char in cand)]))\n",
    "    for key in valid_key:\n",
    "        key_pos.append([x[1] for x in nltk.pos_tag([key][0].split(' '))])\n",
    "    return valid_key,key_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_candidate_words(text_all, good_tags=set(['JJ','JJR','JJS','NN','NNP','NNS','NNPS'])):\n",
    "    punct = set(string.punctuation)\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    candidate_all = []\n",
    "    key_pos = []\n",
    "    for text in sent_tokenize(text_all):\n",
    "        if text!='' and text!=' ':\n",
    "            tagged_words = nltk.pos_tag([word_tokenize(sent) for sent in sent_tokenize(text)][0])\n",
    "            candidates = [word.lower() for word, tag in tagged_words\n",
    "                          if tag in good_tags and word.lower() not in stop_words\n",
    "                          and not all(char in punct for char in word)]\n",
    "            candidate_all += candidates\n",
    "    for key in candidate_all:\n",
    "        key_pos.append([x[1] for x in nltk.pos_tag([key][0].split(' '))])\n",
    "    return candidate_all,key_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def getCandidatePhrases(transcript):\n",
    "    input_ = replaceContractions(transcript)\n",
    "    Keywords_all = list (set (extract_candidate_chunk (transcript) + extract_candidate_words (transcript)))\n",
    "    return Keywords_all\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCandidatePhrases(transcript):\n",
    "    key_pos = {}\n",
    "    transcript = [cleantext(transcript)]\n",
    "    for seg in transcript:\n",
    "        chunk_key,chunk_pos = extract_candidate_chunk (seg)\n",
    "        word_key,word_pos = extract_candidate_words (seg)\n",
    "        key_all = chunk_key + word_key\n",
    "        pos_all = chunk_pos + word_pos\n",
    "        for i in range(len(key_all)):\n",
    "            key_pos[key_all[i]] = pos_all[i]\n",
    "    df = pd.DataFrame({\n",
    "        \"Keyphrase\":list(key_pos.keys()),\n",
    "        \"POS\":list(key_pos.values())\n",
    "    })\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Keyphrase</th>\n",
       "      <th>POS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>course</td>\n",
       "      <td>[NN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>teams</td>\n",
       "      <td>[NNS]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>media analytics ether starts</td>\n",
       "      <td>[NNS, NNS, RB, NNS]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the call happens</td>\n",
       "      <td>[DT, NN, VBZ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>key markers</td>\n",
       "      <td>[JJ, NNS]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ether captures</td>\n",
       "      <td>[NN, NNS]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>a foundation</td>\n",
       "      <td>[DT, NN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>artificial intelligence</td>\n",
       "      <td>[JJ, NN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>the call audio video shared content etc</td>\n",
       "      <td>[DT, NN, NN, NN, VBD, JJ, NN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>top</td>\n",
       "      <td>[NN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>slack stride</td>\n",
       "      <td>[NN, NN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>a smart call service</td>\n",
       "      <td>[DT, JJ, NN, NN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>foundation</td>\n",
       "      <td>[NN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>artificial</td>\n",
       "      <td>[JJ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>intelligence</td>\n",
       "      <td>[NN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>media</td>\n",
       "      <td>[NNS]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>analytics</td>\n",
       "      <td>[NNS]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ether</td>\n",
       "      <td>[NN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>smart</td>\n",
       "      <td>[NN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>call</td>\n",
       "      <td>[NN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>service</td>\n",
       "      <td>[NN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>slack</td>\n",
       "      <td>[NN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>stride</td>\n",
       "      <td>[NN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>captures</td>\n",
       "      <td>[NNS]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>audio</td>\n",
       "      <td>[NN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>video</td>\n",
       "      <td>[NN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>content</td>\n",
       "      <td>[NN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>etc</td>\n",
       "      <td>[NN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>key</td>\n",
       "      <td>[NN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>markers</td>\n",
       "      <td>[NNS]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Keyphrase                            POS\n",
       "0                                    course                           [NN]\n",
       "1                                     teams                          [NNS]\n",
       "2              media analytics ether starts            [NNS, NNS, RB, NNS]\n",
       "3                          the call happens                  [DT, NN, VBZ]\n",
       "4                               key markers                      [JJ, NNS]\n",
       "5                            ether captures                      [NN, NNS]\n",
       "6                              a foundation                       [DT, NN]\n",
       "7                   artificial intelligence                       [JJ, NN]\n",
       "8   the call audio video shared content etc  [DT, NN, NN, NN, VBD, JJ, NN]\n",
       "9                                       top                           [NN]\n",
       "10                             slack stride                       [NN, NN]\n",
       "11                     a smart call service               [DT, JJ, NN, NN]\n",
       "12                               foundation                           [NN]\n",
       "13                               artificial                           [JJ]\n",
       "14                             intelligence                           [NN]\n",
       "15                                    media                          [NNS]\n",
       "16                                analytics                          [NNS]\n",
       "17                                    ether                           [NN]\n",
       "18                                    smart                           [NN]\n",
       "19                                     call                           [NN]\n",
       "20                                  service                           [NN]\n",
       "21                                    slack                           [NN]\n",
       "22                                   stride                           [NN]\n",
       "23                                 captures                          [NNS]\n",
       "24                                    audio                           [NN]\n",
       "25                                    video                           [NN]\n",
       "26                                  content                           [NN]\n",
       "27                                      etc                           [NN]\n",
       "28                                      key                           [NN]\n",
       "29                                  markers                          [NNS]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getCandidatePhrases(\"With a foundation in artificial intelligence and media analytics, Ether starts its course by enabling a smart call service on top of Slack, Stride, and Teams. Ether captures and analyzes the call (audio, video, shared content, etc) as the call happens and extracts key markers.\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "main_language": "python",
   "text_representation": {
    "extension": ".py",
    "format_name": "light"
   }
  },
  "kernelspec": {
   "display_name": "Python [conda env:DL-wpython3]",
   "language": "python",
   "name": "conda-env-DL-wpython3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
